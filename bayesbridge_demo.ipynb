{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesbridge import BayesBridge\n",
    "from simulate_data import simulate_design, simulate_outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesBridge supports both dense (numpy array) and sparse (scipy sparse matrix) design matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs, n_pred = 10 ** 4, 10 ** 3\n",
    "\n",
    "X = simulate_design(\n",
    "    n_obs, n_pred, \n",
    "    corr_dense_design=False,\n",
    "    binary_frac=.6, \n",
    "    binary_pred_freq=.2,\n",
    "    categorical_frac=.3, \n",
    "    n_category=5,\n",
    "    shuffle_columns=True,\n",
    "    format_='sparse',\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signal = 10\n",
    "intercept = 2\n",
    "\n",
    "beta_true = np.zeros(n_pred + 1)\n",
    "beta_true[0] = intercept\n",
    "beta_true[1:(1 + n_signal)] = 1\n",
    "\n",
    "n_trial = np.ones(X.shape[0]) # Binary outcome.\n",
    "y = simulate_outcome(\n",
    "    X, beta_true[1:], intercept=beta_true[0], \n",
    "    n_trial=n_trial, model='logit', seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The regression coefficients corresponding to the first 'n_coef_without_shrinkage' columns of the design matrix are given Gaussian (not shrinkage) priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = BayesBridge(\n",
    "    y, X, model='logit',\n",
    "    add_intercept=True, \n",
    "        # Do not manually add an intercept indicator to X.\n",
    "    n_coef_without_shrinkage=0, \n",
    "        # Number of coefficients without shrinkage i.e. fixed effects\n",
    "    prior_sd_for_intercept=float('inf'),\n",
    "    prior_sd_for_unshrunk=float('inf'),\n",
    "        # Set it to float('inf') for a flat prior.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Gibbs sampler. The BayesBridge uses a prior $ \\pi(\\beta_j \\, | \\, \\tau) \\propto \\tau^{-1} \\exp\\left( - \\, \\left| \\tau^{-1} \\beta_j \\right|^\\alpha \\right) $ for $0 < \\alpha \\leq 1$. The default is $\\alpha = 1 / 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output = bridge.gibbs(\n",
    "    n_burnin=0, n_post_burnin=250, thin=1, \n",
    "    init={'global_shrinkage': .01},\n",
    "    mvnorm_method='cg',\n",
    "    seed=2\n",
    ")\n",
    "samples = mcmc_output['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the convergence by looking at the traceplot for posterior log-density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(samples['logp'])\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.ylabel('Posterior log density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart MCMC from the last iteration with the 'gibbs_additional_iter' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output = bridge.gibbs_additional_iter(\n",
    "    mcmc_output, n_iter=250\n",
    ")\n",
    "samples = mcmc_output['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the convergence again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(samples['logp'])\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.ylabel('Posterior log density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more samples (while keeping the previous ones) with the option 'merge=True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output = bridge.gibbs_additional_iter(\n",
    "    mcmc_output, n_iter=750, merge=True\n",
    ")\n",
    "samples = mcmc_output['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixing of the regression coefficients are adequate for most practical purposes. The mixing rate of the global shrinkage parameter (and hence of the log-density) leaves something to be desired, but at least the convergence is quick. So, we can think of the Gibbs sampler as an improvement over empirical Bayes (if falling short of full Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(samples['beta'][:5, :].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The posterior means of the regression coefficients mostly recover the true values. It seems like there is not enough statistical power, however, and some non-zero coefficients are shrunk toward zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "n_coef_to_plot = 30\n",
    "beta_post_mean = np.mean(samples['beta'], -1)\n",
    "plt.plot(beta_post_mean[:n_coef_to_plot], 'x', ms=10)\n",
    "plt.plot(beta_true[:n_coef_to_plot], '--')\n",
    "plt.xlabel(r'Coefficient index $j$')\n",
    "plt.ylabel(r'$\\beta_j$', rotation=0, labelpad=10)\n",
    "plt.legend(['posterior mean', 'true value'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
